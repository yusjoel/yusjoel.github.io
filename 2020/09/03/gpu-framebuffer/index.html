<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>GPU Framebuffer Memory: Understanding Tiling(WIP) | 小埃拉的博客</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="现代的图形硬件在描绘的操作过程中有大量的内存的带宽需求. 增加外部的内存带宽代价非常的昂贵, 因为需要增加额外的空间和能源, 而对于移动设备的渲染来说尤其困难. 这篇文章要讨论基于图块的渲染, 这种渲染方法在大多数的移动图形硬件上使用, 并且逐步地向桌面硬件发展. 立即模式光栅器传统的图形API给出的接口是将三角形按顺序提交, 然后GPU渲染器依次渲染各个三角形. 光栅化的过程如下图所示: 下面的">
<meta property="og:type" content="article">
<meta property="og:title" content="GPU Framebuffer Memory: Understanding Tiling(WIP)">
<meta property="og:url" content="http://yusjoel.github.io/2020/09/03/gpu-framebuffer/index.html">
<meta property="og:site_name" content="小埃拉的博客">
<meta property="og:description" content="现代的图形硬件在描绘的操作过程中有大量的内存的带宽需求. 增加外部的内存带宽代价非常的昂贵, 因为需要增加额外的空间和能源, 而对于移动设备的渲染来说尤其困难. 这篇文章要讨论基于图块的渲染, 这种渲染方法在大多数的移动图形硬件上使用, 并且逐步地向桌面硬件发展. 立即模式光栅器传统的图形API给出的接口是将三角形按顺序提交, 然后GPU渲染器依次渲染各个三角形. 光栅化的过程如下图所示: 下面的">
<meta property="og:locale">
<meta property="og:image" content="http://yusjoel.github.io/gpu-framebuffer/images/tech_GPUFramebuffer_01.gif">
<meta property="og:image" content="http://yusjoel.github.io/gpu-framebuffer/images/tech_GPUFramebuffer_03-cn.svg">
<meta property="og:image" content="http://yusjoel.github.io/gpu-framebuffer/images/tech_GPUFramebuffer_06.gif">
<meta property="og:image" content="http://yusjoel.github.io/gpu-framebuffer/images/tech_GPUFramebuffer_07.gif">
<meta property="og:image" content="http://yusjoel.github.io/gpu-framebuffer/images/tech_GPUFramebuffer_10.gif">
<meta property="og:image" content="http://yusjoel.github.io/gpu-framebuffer/images/tech_GPUFramebuffer_12.gif">
<meta property="og:image" content="http://yusjoel.github.io/gpu-framebuffer/images/tech_GPUFramebuffer_14.gif">
<meta property="og:image" content="http://yusjoel.github.io/gpu-framebuffer/images/tech_GPUFramebuffer_16-cn.svg">
<meta property="og:image" content="http://yusjoel.github.io/gpu-framebuffer/images/tech_GPUFramebuffer_19.gif">
<meta property="og:image" content="http://yusjoel.github.io/gpu-framebuffer/images/tech_GPUFramebuffer_21.gif">
<meta property="og:image" content="http://yusjoel.github.io/gpu-framebuffer/images/tech_GPUFramebuffer_23.gif">
<meta property="article:published_time" content="2020-09-03T02:43:09.000Z">
<meta property="article:modified_time" content="2020-09-11T10:19:54.472Z">
<meta property="article:author" content="Joel">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yusjoel.github.io/gpu-framebuffer/images/tech_GPUFramebuffer_01.gif">
  
    <link rel="alternate" href="/atom.xml" title="小埃拉的博客" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 5.1.1"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">小埃拉的博客</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yusjoel.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-gpu-framebuffer" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/09/03/gpu-framebuffer/" class="article-date">
  <time datetime="2020-09-03T02:43:09.000Z" itemprop="datePublished">2020-09-03</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      GPU Framebuffer Memory: Understanding Tiling(WIP)
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>现代的图形硬件在描绘的操作过程中有大量的内存的带宽需求. 增加外部的内存带宽代价非常的昂贵, 因为需要增加额外的空间和能源, 而对于移动设备的渲染来说尤其困难. 这篇文章要讨论基于图块的渲染, 这种渲染方法在大多数的移动图形硬件上使用, 并且逐步地向桌面硬件发展.</p>
<h2 id="立即模式光栅器"><a href="#立即模式光栅器" class="headerlink" title="立即模式光栅器"></a>立即模式光栅器</h2><p>传统的图形API给出的接口是将三角形按顺序提交, 然后GPU渲染器依次渲染各个三角形. 光栅化的过程如下图所示:</p>
<p><em>下面的图片, 包括之后所有的图片, 都是左侧显示颜色缓冲, 右侧显示深度缓冲</em></p>
<p><img src="/gpu-framebuffer/images/tech_GPUFramebuffer_01.gif" alt="简单的立即模式渲染过程"><br>简单的立即模式渲染过程</p>
<p>这些三角形一提交就会被硬件处理, 如上图所示, 称之为立即模式渲染器(IMR). 过去, 桌面和主机的GPU的做法都可以概略地认为是这样.</p>
<p><em>在立即模式渲染器中, 图形渲染管线从上至下地处理各个原语, 逐原语的方式访问内存.</em></p>
<p><img src="/gpu-framebuffer/images/tech_GPUFramebuffer_03-cn.svg" alt="IMR管线"><br>IMR管线</p>
<h2 id="立即模式渲染器的内存使用"><a href="#立即模式渲染器的内存使用" class="headerlink" title="立即模式渲染器的内存使用"></a>立即模式渲染器的内存使用</h2><p>一个单纯的IMR的实现可能会花费大量的内存带宽. 下面这张图展示了即便对帧缓冲的颜色与深度做了一个简单的缓存, 也会造成光栅化过程中大量的内存数据传送. IMR在访问内存时的顺序是不可预测的, 取决于三角形提交的顺序.</p>
<p><em>在下面这张图中, 图片的上方显示了内存中连续4个的缓存行在渲染过程中的情况. 在每个缓存行上方有一个小的矩形, 代表这个缓存行落在了帧缓冲的哪个位置: 红色线条代表缓存行被写入, 处于脏的状态, 绿色代表数据和内存一致, 处于干净的状态, 随着写入的时间推移, 红色会越来越浅. 而在下方的帧缓冲图像中, 颜色缓冲上粉红色代表脏的缓存行, 深度缓冲中则是用白色来代表.</em></p>
<p><img src="/gpu-framebuffer/images/tech_GPUFramebuffer_06.gif" alt="使用缓存行进行渲染"><br>使用缓存行进行渲染</p>
<h2 id="图块化内存"><a href="#图块化内存" class="headerlink" title="图块化内存"></a>图块化内存</h2><p>减少内存带宽的第一步是把每个缓存行覆盖内存中一个块二维的区域(一个图块). 在空间中相互邻近的三角形往往也会一起提交, 在上面这个例子中, 可以看到物件的每个尖刺都是先整体描绘完才去画下一个的, 更好地对缓存区域进行分组能够获得更好的命中率. 使用正方形的缓存区域, 总面积和线型区域是一样的, 但是会囊括更多的描绘, 就能减少内存数据的传输的频繁程度, 这样就能减少额外的带宽了! 相同的技术也常用于纹理的存储, 因为纹理数据的读取也表现除了空间上的引用局部性.</p>
<p>这个例子是被简化过的 - 现在的硬件会使用更复杂的像素与内存之间的映射机制来提高引用局部性.</p>
<p><em>如下图所示, 此时的4个缓存行分别覆盖帧缓冲和深度缓冲上的一块正方形区域. 上方显示了覆盖区域在两个缓冲中的位置. 这些缓存行覆盖的像素和之前是一样多的.</em></p>
<p><img src="/gpu-framebuffer/images/tech_GPUFramebuffer_07.gif" alt="使用正方形的缓存图块进行渲染"><br>使用正方形的缓存图块进行渲染</p>
<h2 id="使用图块进行光栅化"><a href="#使用图块进行光栅化" class="headerlink" title="使用图块进行光栅化"></a>使用图块进行光栅化</h2><p>在实际情况中, 帧缓冲会比缓存图块大很多. 当要渲染一个大的三角形, 并且简单地从上之下地那么逐行渲染, 就会造成缓存抖动, 因为屏幕上的一条水平线所包含的图块要比缓存中的图块来的多. 要解决这个问题, 我们可以改变三角形中像素的光栅化顺序: 我们可以先把一个图块中属于这个三角形的所有像素先画出来, 然后再移动到下一个图块.</p>
<p>由于在这个简单的例子中, 缓存的图块正好能填满帧缓冲的宽, 所以使用这种方式进行光栅化并不能减少内存数据的传输数量. 但是, 我们能看到两者之间的区别 - 先把一个缓存图块内的三角形部分画完, 再去处理下一个图块.</p>
<p><em>下面这个动画花费的时间比上一个动画长, 因为这个动画在三角形内的一个图块更新时都会截取一帧, 而上一个动画则是在一个三角形完成渲染后或者图块与内存之间进行了数据传输后才会截取一帧. 在真实的硬件上, 两者的性能应该是一致的, 并且, 如果上一个方法造成了缓存的抖动, 那么这个版本的性能会更优.</em></p>
<p><img src="/gpu-framebuffer/images/tech_GPUFramebuffer_10.gif" alt="一次渲染一个图块"><br>一次渲染一个图块</p>
<p>我们还可以继续优化对内存的访问, 不要在处理完当前图块的一个三角形内的像素就处理下一个图块, 可以把场景中所有的三角形都处理完, 再移动到下一个图块. 这个就是基于图块的渲染器(TBR)所使用的优化方法.</p>
<h2 id="分箱-Binning"><a href="#分箱-Binning" class="headerlink" title="分箱(Binning)"></a>分箱(Binning)</h2><p>TBR的第一步就是确定每个图块分别受到哪些三角形的影响. 一个TBR的最原始的实现是对每个图块把场景中的所有三角形渲染一遍, 再将超出图块的部分裁剪掉. 但在实际使用的时候, 帧缓冲相对于图块来说非常得大, 这个方法非常的没有效率. 取而代之的方法是, 当一个三角形被提交, 先不立即对它进行光栅化, 而是分箱到一个内存中的结构中, 这个结构定义了它影响到了哪些图块. 注意, 这个操作包含了顶点着色, 因为这和三角形的位置相关, 不过和片元着色无关.</p>
<p><em>下面这张图演示了场景中的各个三角形是如何分箱到12个图块中, 整个帧缓冲正好可以分成4x3个图块. 下方的帧缓冲显示当前提交的三角形, 上方则按照4x3的方式排列着12个缩小的帧缓冲, 每个帧缓冲只显示被分箱到对应图块的三角形, 图块对应的区域由一条红线框出.</em></p>
<p><img src="/gpu-framebuffer/images/tech_GPUFramebuffer_12.gif" alt="将三角形分箱"><br>将三角形分箱</p>
<h2 id="基于图块的光栅化"><a href="#基于图块的光栅化" class="headerlink" title="基于图块的光栅化"></a>基于图块的光栅化</h2><p>当三角形完成分箱后, 光栅器就可以按箱来进行处理, 每次只对一个图块的内存进行写入, 直到这个图块处理完毕. 由于每个图块只处理一次, 那么缓存就减少到了一个图块那么大. 这个顺序操作包含了清空帧缓冲, 在图块处理过程中, 整个帧缓冲都是脏的.</p>
<p><img src="/gpu-framebuffer/images/tech_GPUFramebuffer_14.gif" alt="按图块进行渲染"><br>按图块进行渲染</p>
<p><em>渲染分成了两个阶段: 分箱, 这步需要写内存; 光栅化, 需要读取箱内数据. 几何数据的中间存储相对于帧缓冲一般而言是更小的, 并且会顺序地进行访问.</em></p>
<p><img src="/gpu-framebuffer/images/tech_GPUFramebuffer_16-cn.svg"></p>
<p>由于需要等到所有的几何数据提交完毕才能开始光栅化, 所以和立即模式相比, 基于图块的渲染会有一个延迟. 而这个延迟带来的回报则是减少带宽, 增快光栅化速度. 在某些TBR的硬件上, 分箱和光栅化是流水线化的. 因此, 任何限制了并行性的操作都会造成性能问题, 比如说一个顶点着色器需要使用前一帧的输出, 一个纹理每一帧都会修改, 并且没有使用双缓冲. 另外, 某些TBR的硬件限制了分箱阶段的几何数据的数量.</p>
<p>尽管如此, 带宽的节省对于移动设备来说还是最重要的, 所以几乎所有的移动设备都使用了TBR. 甚至传统的桌面IMR供应商也在他们的新硬件中部分地使用基于图块的方法. 这意味着桌面和移动端都能在新的支持图块的API中收益, 如Vulkan Subpasses.</p>
<p>Since we process all the geometry contributing to the image one tile at a time, it may not be necessary to read any previous value from the framebuffer - we can clear the image as part of the tile processing (as shown above) and avoid the bandwidth cost of a read unless we really need previous contents. It is often also possible to avoid writing the depth buffer to memory (not shown in the above example), since typically the depth value is only used during rendering and does not need to persist between frames.</p>
<p>External traffic to the framebuffer is now limited to one write per tile - although these writes include clearing the framebuffer to a background color when no other primitives were there.</p>
<h2 id="Multisampling"><a href="#Multisampling" class="headerlink" title="Multisampling"></a>Multisampling</h2><p>Tile-based rendering also provides a low-bandwidth way to implement antialiasing: we can render to the tiles normally, and average pixel values as part of the operation of writing the tile memory. This downsampling step is known as “resolving” the tile buffer. When multisampling (as opposed to supersampling), not every on-chip pixel is shaded.</p>
<p>If the tile buffer is of a fixed size, antialiasing means the image must be divided into more tiles, and there are more writes from tile memory to the framebuffer - but the total amount of memory transferred to the framebuffer is unaffected by the degree of multisampling. The full-resolution version of the framebuffer (the version that has not been downsampled) never needs to be written to memory, so long as no further processing is done to the same render target. This can save a lot of bandwidth, and for simple scenes makes multisampling almost free.</p>
<p><em>In this animation, 2x2 antialiasing has made the tile coverage in the framebuffer smaller, so more passes are needed. The geometry rasterized in the tile memory is double-sized, and shrunk when written to the framebuffer. The depth buffer is only needed on-chip, not in main memory, so only the color aspect of the full framebuffer is shown - the on-chip depth value is discarded once the tile is processed.</em></p>
<p><img src="/gpu-framebuffer/images/tech_GPUFramebuffer_19.gif" alt="Multisampled tiled rendering"><br>Multisampled tiled rendering</p>
<h2 id="Traditional-deferred-shading"><a href="#Traditional-deferred-shading" class="headerlink" title="Traditional deferred shading"></a>Traditional deferred shading</h2><p>It is not normally possible to read from the framebuffer attachment during the process of rendering to it. Nonetheless, some techniques rely on being able to read back the result of previous rendering operations.</p>
<p>One such technique is “deferred rendering”: only basic information is recorded as each primitive is rasterized, then a second pass is made over the rendered scene, using this recorded information as input to the shading operations per pixel. Deferred rendering can reduce the number of required costly state changes, and increase the potential parallelism available to fragment shaders.</p>
<p>A simple implementation of deferred rendering has a high bandwidth cost, since the entire framebuffer, including all per-pixel values, must be read and written for the deferred shading pass.</p>
<p><em>This example shows the cache behavior in a simple deferred shading implementation. The first pass simply records the Phong-interpolated surface normal. The second pass reads this information for every pixel in the image and uses the interpolated normal for lighting calculations - reading and writing every image line in the process.</em></p>
<p><img src="/gpu-framebuffer/images/tech_GPUFramebuffer_21.gif" alt="Deferred shading with an IMR"><br>Deferred shading with an IMR</p>
<h2 id="Tiling-and-deferred-shading"><a href="#Tiling-and-deferred-shading" class="headerlink" title="Tiling and deferred shading"></a>Tiling and deferred shading</h2><p>Because deferred shading (and the related deferred lighting approach) require only the contents of the current pixel to be read, the scene can still be processed one tile at a time. Only the final result of the shading pass needs to be written to the framebuffer.</p>
<p>To achieve this in Vulkan, the entire sequence of rendering is treated as a single render pass, and the geometry and shading operations are each contained in a subpass. In OpenGL ES, a similar approach is possible less formally with Pixel Local Storage. With these approaches, the memory access cost of deferred shading is no greater than for simple rendering - and there is still no need to write the depth buffer.</p>
<p><em>This example shows deferred shading in a tile-based renderer: the triangle rasterisation and the subsequent shading pass proceed within the tile memory, with only the RGB shading result being written out to the framebuffer.</em></p>
<p><img src="/gpu-framebuffer/images/tech_GPUFramebuffer_23.gif" alt="Tiled deferred shading"><br>Tiled deferred shading</p>
<h2 id="Advantages-of-tile-based-rendering"><a href="#Advantages-of-tile-based-rendering" class="headerlink" title="Advantages of tile-based rendering"></a>Advantages of tile-based rendering</h2><ul>
<li><p>Frame buffer memory bandwidth is greatly reduced, reducing power and increasing speed.</p>
<pre><code>*   Mobile memory is typically slower and lower power than desktop systems, and bandwidth is shared with the CPU, so access is very costly.</code></pre>
</li>
<li><p>With API support, off-chip memory requirements may also be reduced (it may not be necessary to allocate an off-chip Z buffer at all, for example).</p>
</li>
<li><p>Texture cache performance can be improved (textures covering multiple primitives may be accessed more coherently one tile at a time than one primitive at a time.</p>
</li>
<li><p>Much less on-chip space is needed for good performance compared with a general-purpose frame buffer cache.</p>
<pre><code>*   This means that more space can be dedicated to texture cache, further reducing bandwidth.</code></pre>
</li>
</ul>
<h2 id="Limitations-of-tile-based-rendering"><a href="#Limitations-of-tile-based-rendering" class="headerlink" title="Limitations of tile-based rendering"></a>Limitations of tile-based rendering</h2><p>While there are many performance advantages to tile-based rendering, there are some restrictions imposed by the technique:</p>
<ul>
<li><p>The two-stage binning and fragment passes introduce latency</p>
<pre><code>*   This latency should be hidden by pipelining and improved performance, but makes some operations relatively more costly

*   In pipelined tiled rendering, framebuffer and textures required for rendering should be double-buffered so as to avoid stalling the pipeline</code></pre>
</li>
<li><p>Framebuffer reads that might fall outside the current fragment are relatively more costly</p>
<pre><code>*   Operations such as screen-space ray tracing require writing all the framebuffer data - removing the ability to discard full-resolution images and depth values after use</code></pre>
</li>
<li><p>There is a cost to traversing the geometry repeatedly</p>
<pre><code>*   Scenes that are vertex-shader bound may have increased overhead in a tiler</code></pre>
</li>
<li><p>The binning pass may have limitations</p>
<pre><code>*   Some implementations may run out of space for binning primitives in very complex scenes, or may have optimizations that are bypassed by unusual input (such as highly irregular geometry)</code></pre>
</li>
<li><p>Switching to a different render target and back involves flushing all working data to memory and later reading it back</p>
<pre><code>*   For a tiler, it is especially important that shadow and environment maps be generated before the main frame buffer, not &quot;on demand&quot; during final rendering (though this is good advice for most GPUs)</code></pre>
</li>
<li><p>Graphics state (such as shaders) may change more frequently and less predictably</p>
<pre><code>*   Geometry that is &quot;skipped&quot; means that states do not necessarily follow in turn, making incremental state updates hard to implement</code></pre>
</li>
</ul>
<p>In most cases, the behavior of a tile-based GPU should not be appreciably worse than for an immediate-mode renderer using similarly limited hardware (indeed, some hardware can choose whether or not to run in a tiled mode), but it is possible to remove the performance benefits of tile-based rendering with the wrong use pattern.</p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Tile-based rendering is a technique used by modern GPUs to reduce the bandwidth requirements of accessing off-chip framebuffer memory. Almost ubiquitous in the mobile space where external memory access is costly and rendering demands have historically been lower, desktop GPUs are now beginning to make use of partially-tile-based rendering as well.</p>
<p>Vulkan has specific features intended to make the best use of tile-based renderers, including control over whether to load or clear previous framebuffer content, whether to discard or write attachment contents and control over attachment resolving, and subpasses. OpenGL ES can achieve similar behavior with extensions, but these are not universally supported. To get the best performance from current and future GPUs, it is important to make proper use of the API so that tile-based rendering can proceed efficiently.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yusjoel.github.io/2020/09/03/gpu-framebuffer/" data-id="ckerwpzw40003r4rqa0pqgcxa" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">September 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/09/03/gpu-framebuffer/">GPU Framebuffer Memory: Understanding Tiling(WIP)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 Joel<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>