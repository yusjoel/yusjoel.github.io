<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>小埃拉的博客</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="小埃拉的博客">
<meta property="og:url" content="http://yusjoel.github.io/index.html">
<meta property="og:site_name" content="小埃拉的博客">
<meta property="og:locale">
<meta property="article:author" content="Joel">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="小埃拉的博客" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 5.1.1"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">小埃拉的博客</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yusjoel.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-gpu-framebuffer" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/09/03/gpu-framebuffer/" class="article-date">
  <time datetime="2020-09-03T02:43:09.000Z" itemprop="datePublished">2020-09-03</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/09/03/gpu-framebuffer/">GPU Framebuffer Memory: Understanding Tiling(WIP)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>现代的图形硬件在描绘的操作过程中有大量的内存的带宽需求. 增加外部的内存带宽代价非常的昂贵, 因为需要增加额外的空间和能源, 而对于移动设备的渲染来说尤其困难. 这篇文章要讨论基于图块的渲染, 这种渲染方法在大多数的移动图形硬件上使用, 并且逐步地向桌面硬件发展.</p>
<h2 id="立即模式光栅器"><a href="#立即模式光栅器" class="headerlink" title="立即模式光栅器"></a>立即模式光栅器</h2><p>传统的图形API给出的接口是将三角形按顺序提交, 然后GPU渲染器依次渲染各个三角形. 光栅化的过程如下图所示:</p>
<p><em>下面的图片, 包括之后所有的图片, 都是左侧显示颜色缓冲, 右侧显示深度缓冲</em></p>
<p><img src="/gpu-framebuffer/images/tech_GPUFramebuffer_01.gif" alt="Simple "><br>简单的立即模式渲染过程</p>
<p>这些三角形一提交就会被硬件处理, 如上图所示, 称之为立即模式渲染器(IMR). 过去, 桌面和主机的GPU的做法都可以概略地认为是这样.</p>
<p><em>在立即模式渲染器中, 图形渲染管线从上至下地处理各个原语, 逐原语的方式访问内存.</em></p>
<p><img src="/gpu-framebuffer/images/tech_GPUFramebuffer_03.svg" alt="Pipeline of an "><br>IMR管线</p>
<h2 id="立即模式渲染器的内存使用"><a href="#立即模式渲染器的内存使用" class="headerlink" title="立即模式渲染器的内存使用"></a>立即模式渲染器的内存使用</h2><p>一个单纯的IMR的实现可能会花费大量的内存带宽. 下面这张图展示了即便对帧缓存的颜色与深度做了一个简单的缓冲, 也会造成光栅化过程中大量的内存数据传送. IMR在访问内存时的顺序是不可预测的, 取决于三角形提交的顺序.</p>
<p><em>在下面这张图中, 图片的上方显示了内存中连续4条的缓冲线在渲染过程中的情况. 在每条缓冲线上方有一个小的矩形, 代表这个缓冲线落在了帧缓存的哪个位置: 红色线条代表缓冲线被写入, 处于脏的状态, 绿色代表数据和内存一致, 处于干净的状态, 随着写入的时间推移, 红色会越来越浅. 而在下方的帧缓存图像中, 颜色缓存上粉红色代表脏的缓冲线, 深度缓存中则是用白色来代表.</em></p>
<p><img src="/gpu-framebuffer/images/tech_GPUFramebuffer_06.gif" alt="Rendering with linear cachelines"><br>Rendering with linear cachelines</p>
<h2 id="图块化内存"><a href="#图块化内存" class="headerlink" title="图块化内存"></a>图块化内存</h2><p>减少内存带宽的第一步是把每个缓冲线覆盖内存中一个块二维的区域(一个图块). 在空间中相互邻近的三角形往往也会一起提交The first step towards reducing memory bandwidth is to treat each cache line as covering a two-dimensional rectangular area (a “tile”) in memory. Triangles that are near to each other in space are often submitted near each other in time (in this example, each “spike” of the object is drawn before moving on to the next), so better grouping of the cache area results in more cache hits. With square cache areas that are the same size as a linear cache, more rendering happens within the cache, and transfers to memory are less frequent - we’ve reduced external memory bandwidth! A similar technique is often used in texture storage, since the reading of texture values similarly shows spatial locality of reference.</p>
<p>This example is simplified - actual hardware may use more complex mappings between pixels and memory in order to further improve locality of reference.</p>
<p><em>This time the four cache lines (spaced out horizontally) each cover a square area in the framebuffer and depth buffer. The cached framebuffer area is now shown above the corresponding cached depth buffer area. The cache lines hold the same number of pixels as for the linear cache in the previous example.</em></p>
<p><img src="/gpu-framebuffer/images/tech_GPUFramebuffer_07.gif" alt="Rendering with square cache tiles"><br>Rendering with square cache tiles</p>
<h2 id="Rasterizing-within-tiles"><a href="#Rasterizing-within-tiles" class="headerlink" title="Rasterizing within tiles"></a>Rasterizing within tiles</h2><p>In a real-world situation, the framebuffer would likely be larger relative to the cached tiles. One problem with the technique we’ve shown so far is that a large triangle might thrash the cache if drawn in simple top-to-bottom order, since each horizontal line on the screen might cover more tiles than can fit in the cache. We can solve this problem by changing the order in which the pixels within a triangle are rasterized: we can draw all the pixels that the triangle covers within one tile before moving on to the next tile.</p>
<p>Since in our simple example the cache can cover the entire framebuffer width, this approach doesn’t reduce the number of memory transfers that are performed. However, we can see the difference - rendering for a triangle completes in one cached tile before moving to the next.</p>
<p><em>Note: This animation takes longer than the last one because the image is updated after each tile has processed a triangle; the previous animation updated only after each triangle was completely rendered and during transfers between tiles an memory. In real hardware, performance would be the same - and, if the previous approach caused the cache to thrash, the performance of this version would be better.</em></p>
<p><img src="/gpu-framebuffer/images/tech_GPUFramebuffer_10.gif" alt="Rendering a tile at a time"><br>Rendering a tile at a time</p>
<p>We would get even better memory access if, rather than just processing all the pixels corresponding to one triangle before moving on to the next tile, we processed the pixels for all the triangles in the scene. This is the optimization performed by tile-based renderers (or “TBRs”).</p>
<h2 id="Binning"><a href="#Binning" class="headerlink" title="Binning"></a>Binning</h2><p>The first step of tile-based rendering is to determine which triangles affect each tile. A primitive implementation of a tile-based renderer could simply render the entire scene for each tile, clipped to the area covered by the cache. In practice, with large framebuffers and relatively small tiles, this would be very inefficient. Instead, when geometry is submitted for rendering, rather than being immediately rasterized, it is “binned” to a structure in memory that determines which tiles it could affect. Note that this process involves vertex shading, since this affects the location of triangles, but not fragment shading.</p>
<p><em>This diagram shows each triangle of our scene being “binned” into twelve tiles that cover the frame buffer in a 4x3 pattern. The “framebuffer” at the bottom shows the triangle as it is submitted. Above the full-size framebuffer is a 4x3 arrangement of miniature framebuffers, each showing only the triangles that have been “binned” into the corresponding rectangular tile of the image; the area corresponding to the tile is faintly outlined.</em></p>
<p><img src="/gpu-framebuffer/images/tech_GPUFramebuffer_12.gif" alt="Sorting triangles into bins"><br>Sorting triangles into bins</p>
<h2 id="Tile-based-rasterization"><a href="#Tile-based-rasterization" class="headerlink" title="Tile-based rasterization"></a>Tile-based rasterization</h2><p>Once the geometry has been sorted into bins, the rasterizer can process the scene one bin at a time, writing only to local tile memory until processing of the tile is finished. Since each tile is processed only once, the “cache” is now reduced to a single tile. This in-order processing includes clearing the framebuffer - all of the framebuffer is “dirty” until the tile is processed.</p>
<p><img src="/gpu-framebuffer/images/tech_GPUFramebuffer_14.gif" alt="Tiled rendering"><br>Tiled rendering</p>
<p><em>Rendering has now been broken into two stages: binning, which writes to memory, then rasterization, that reads the bin contents. The intermediate store of geometry is typically small relative to the framebuffer, and is accessed in an ordered way.</em></p>
<p><img src="/gpu-framebuffer/images/tech_GPUFramebuffer_16.svg"></p>
<p>Because the rasterization of tiles cannot begin until all the geometry has been processed, tile-based rendering introduces latency compared with immediate-mode rendering. In return, the reduction in bandwidth increases the rasterization speed. In some tile-based rendering hardware, the binning and rasterizing operations are pipelined. Therefore any operation that limits this parallelism (such as a vertex shader that depends on the previous frame’s output, or a texture which is modified frame-by-frame and is not double-buffered) introduces a “bubble” that reduces performance. Additionally some tile-based rendering hardware is limited in the amount of geometry which can be processed in the binning pass.</p>
<p>Nonetheless, the bandwidth reduction of tile-based rendering means that almost all mobile hardware is based on tiling. Even traditional desktop IMR vendors are moving towards a partly tiled approach with their latest hardware. This means that both desktop and mobile platforms can benefit from API changes designed to support tillers (such as Vulkan Subpasses).</p>
<p>Since we process all the geometry contributing to the image one tile at a time, it may not be necessary to read any previous value from the framebuffer - we can clear the image as part of the tile processing (as shown above) and avoid the bandwidth cost of a read unless we really need previous contents. It is often also possible to avoid writing the depth buffer to memory (not shown in the above example), since typically the depth value is only used during rendering and does not need to persist between frames.</p>
<p>External traffic to the framebuffer is now limited to one write per tile - although these writes include clearing the framebuffer to a background color when no other primitives were there.</p>
<h2 id="Multisampling"><a href="#Multisampling" class="headerlink" title="Multisampling"></a>Multisampling</h2><p>Tile-based rendering also provides a low-bandwidth way to implement antialiasing: we can render to the tiles normally, and average pixel values as part of the operation of writing the tile memory. This downsampling step is known as “resolving” the tile buffer. When multisampling (as opposed to supersampling), not every on-chip pixel is shaded.</p>
<p>If the tile buffer is of a fixed size, antialiasing means the image must be divided into more tiles, and there are more writes from tile memory to the framebuffer - but the total amount of memory transferred to the framebuffer is unaffected by the degree of multisampling. The full-resolution version of the framebuffer (the version that has not been downsampled) never needs to be written to memory, so long as no further processing is done to the same render target. This can save a lot of bandwidth, and for simple scenes makes multisampling almost free.</p>
<p><em>In this animation, 2x2 antialiasing has made the tile coverage in the framebuffer smaller, so more passes are needed. The geometry rasterized in the tile memory is double-sized, and shrunk when written to the framebuffer. The depth buffer is only needed on-chip, not in main memory, so only the color aspect of the full framebuffer is shown - the on-chip depth value is discarded once the tile is processed.</em></p>
<p><img src="/gpu-framebuffer/images/tech_GPUFramebuffer_19.gif" alt="Multisampled tiled rendering"><br>Multisampled tiled rendering</p>
<h2 id="Traditional-deferred-shading"><a href="#Traditional-deferred-shading" class="headerlink" title="Traditional deferred shading"></a>Traditional deferred shading</h2><p>It is not normally possible to read from the framebuffer attachment during the process of rendering to it. Nonetheless, some techniques rely on being able to read back the result of previous rendering operations.</p>
<p>One such technique is “deferred rendering”: only basic information is recorded as each primitive is rasterized, then a second pass is made over the rendered scene, using this recorded information as input to the shading operations per pixel. Deferred rendering can reduce the number of required costly state changes, and increase the potential parallelism available to fragment shaders.</p>
<p>A simple implementation of deferred rendering has a high bandwidth cost, since the entire framebuffer, including all per-pixel values, must be read and written for the deferred shading pass.</p>
<p><em>This example shows the cache behavior in a simple deferred shading implementation. The first pass simply records the Phong-interpolated surface normal. The second pass reads this information for every pixel in the image and uses the interpolated normal for lighting calculations - reading and writing every image line in the process.</em></p>
<p><img src="/gpu-framebuffer/images/tech_GPUFramebuffer_21.gif" alt="Deferred shading with an IMR"><br>Deferred shading with an IMR</p>
<h2 id="Tiling-and-deferred-shading"><a href="#Tiling-and-deferred-shading" class="headerlink" title="Tiling and deferred shading"></a>Tiling and deferred shading</h2><p>Because deferred shading (and the related deferred lighting approach) require only the contents of the current pixel to be read, the scene can still be processed one tile at a time. Only the final result of the shading pass needs to be written to the framebuffer.</p>
<p>To achieve this in Vulkan, the entire sequence of rendering is treated as a single render pass, and the geometry and shading operations are each contained in a subpass. In OpenGL ES, a similar approach is possible less formally with Pixel Local Storage. With these approaches, the memory access cost of deferred shading is no greater than for simple rendering - and there is still no need to write the depth buffer.</p>
<p><em>This example shows deferred shading in a tile-based renderer: the triangle rasterisation and the subsequent shading pass proceed within the tile memory, with only the RGB shading result being written out to the framebuffer.</em></p>
<p><img src="/gpu-framebuffer/images/tech_GPUFramebuffer_23.gif" alt="Tiled deferred shading"><br>Tiled deferred shading</p>
<h2 id="Advantages-of-tile-based-rendering"><a href="#Advantages-of-tile-based-rendering" class="headerlink" title="Advantages of tile-based rendering"></a>Advantages of tile-based rendering</h2><ul>
<li><p>Frame buffer memory bandwidth is greatly reduced, reducing power and increasing speed.</p>
<pre><code>*   Mobile memory is typically slower and lower power than desktop systems, and bandwidth is shared with the CPU, so access is very costly.</code></pre>
</li>
<li><p>With API support, off-chip memory requirements may also be reduced (it may not be necessary to allocate an off-chip Z buffer at all, for example).</p>
</li>
<li><p>Texture cache performance can be improved (textures covering multiple primitives may be accessed more coherently one tile at a time than one primitive at a time.</p>
</li>
<li><p>Much less on-chip space is needed for good performance compared with a general-purpose frame buffer cache.</p>
<pre><code>*   This means that more space can be dedicated to texture cache, further reducing bandwidth.</code></pre>
</li>
</ul>
<h2 id="Limitations-of-tile-based-rendering"><a href="#Limitations-of-tile-based-rendering" class="headerlink" title="Limitations of tile-based rendering"></a>Limitations of tile-based rendering</h2><p>While there are many performance advantages to tile-based rendering, there are some restrictions imposed by the technique:</p>
<ul>
<li><p>The two-stage binning and fragment passes introduce latency</p>
<pre><code>*   This latency should be hidden by pipelining and improved performance, but makes some operations relatively more costly

*   In pipelined tiled rendering, framebuffer and textures required for rendering should be double-buffered so as to avoid stalling the pipeline</code></pre>
</li>
<li><p>Framebuffer reads that might fall outside the current fragment are relatively more costly</p>
<pre><code>*   Operations such as screen-space ray tracing require writing all the framebuffer data - removing the ability to discard full-resolution images and depth values after use</code></pre>
</li>
<li><p>There is a cost to traversing the geometry repeatedly</p>
<pre><code>*   Scenes that are vertex-shader bound may have increased overhead in a tiler</code></pre>
</li>
<li><p>The binning pass may have limitations</p>
<pre><code>*   Some implementations may run out of space for binning primitives in very complex scenes, or may have optimizations that are bypassed by unusual input (such as highly irregular geometry)</code></pre>
</li>
<li><p>Switching to a different render target and back involves flushing all working data to memory and later reading it back</p>
<pre><code>*   For a tiler, it is especially important that shadow and environment maps be generated before the main frame buffer, not &quot;on demand&quot; during final rendering (though this is good advice for most GPUs)</code></pre>
</li>
<li><p>Graphics state (such as shaders) may change more frequently and less predictably</p>
<pre><code>*   Geometry that is &quot;skipped&quot; means that states do not necessarily follow in turn, making incremental state updates hard to implement</code></pre>
</li>
</ul>
<p>In most cases, the behavior of a tile-based GPU should not be appreciably worse than for an immediate-mode renderer using similarly limited hardware (indeed, some hardware can choose whether or not to run in a tiled mode), but it is possible to remove the performance benefits of tile-based rendering with the wrong use pattern.</p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Tile-based rendering is a technique used by modern GPUs to reduce the bandwidth requirements of accessing off-chip framebuffer memory. Almost ubiquitous in the mobile space where external memory access is costly and rendering demands have historically been lower, desktop GPUs are now beginning to make use of partially-tile-based rendering as well.</p>
<p>Vulkan has specific features intended to make the best use of tile-based renderers, including control over whether to load or clear previous framebuffer content, whether to discard or write attachment contents and control over attachment resolving, and subpasses. OpenGL ES can achieve similar behavior with extensions, but these are not universally supported. To get the best performance from current and future GPUs, it is important to make proper use of the API so that tile-based rendering can proceed efficiently.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yusjoel.github.io/2020/09/03/gpu-framebuffer/" data-id="ckem865gs0000h0rq4xyp4qxp" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">September 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/09/03/gpu-framebuffer/">GPU Framebuffer Memory: Understanding Tiling(WIP)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 Joel<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>